{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "69acd3c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/workspaces/Kaggle-Solution-Finder-Agent/app/.venv/bin/python\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "print(sys.executable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0a0e8c52",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspaces/Kaggle-Solution-Finder-Agent/app/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# 1) Redirect logs to eval_logs/\n",
    "os.environ[\"LOGS_DIRECTORY\"] = \"eval_logs\"\n",
    "\n",
    "# 2) Make app/ importable\n",
    "REPO_ROOT = Path.cwd().parent\n",
    "sys.path.insert(0, str(REPO_ROOT / \"app\"))\n",
    "\n",
    "from ingest import (read_repo_data,extract_completed_competitions,build_vector_index)\n",
    "from search_agent import create_search_agent\n",
    "from logs import log_interaction_to_file, LOG_DIR, evaluate_log_record, load_log_file, simplify_log_messages\n",
    "from pydantic_ai import Agent\n",
    "from logs import EvaluationChecklist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "91356668",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PosixPath('/workspaces/Kaggle-Solution-Finder-Agent/eval/logs')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "LOG_DIR.absolute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1727e8fa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Agent(model=OpenAIChatModel(), name='eval_agent', end_strategy='early', model_settings=None, output_type=<class '__main__.EvaluationChecklist'>, instrument=None)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluation_prompt = \"\"\"\n",
    "Use this checklist to evaluate the quality of an AI agent's answer (<ANSWER>) to a user question (<QUESTION>).\n",
    "    We also include the entire log (<LOG>) for analysis.\n",
    "\n",
    "    For each item, check if the condition is met. \n",
    "\n",
    "    Checklist:\n",
    "\n",
    "    - instructions_follow: The agent followed the user's instructions (in <INSTRUCTIONS>)\n",
    "    - instructions_avoid: The agent avoided doing things it was told not to do  \n",
    "    - answer_relevant: The response directly addresses the user's question  \n",
    "    - answer_clear: The answer is clear and correct  \n",
    "    - answer_citations: The response includes proper citations or sources when required  \n",
    "    - completeness: The response is complete and covers all key aspects of the request\n",
    "    - tool_call_search: Is the search tool invoked? \n",
    "\n",
    "    Output true/false for each check and provide a short explanation for your judgment.\n",
    "\"\"\".strip()\n",
    "\n",
    "from pydantic import BaseModel\n",
    "from pydantic_ai import Agent\n",
    "\n",
    "class EvaluationCheck(BaseModel):\n",
    "    check_name: str\n",
    "    justification: str\n",
    "    check_pass: bool\n",
    "\n",
    "class EvaluationChecklist(BaseModel):\n",
    "    checklist: list[EvaluationCheck]\n",
    "    summary: str\n",
    "    \n",
    "eval_agent = Agent(name = 'eval_agent',model='gpt-5-nano',\n",
    "                       instructions=evaluation_prompt,\n",
    "                       output_type=EvaluationChecklist)\n",
    "                    \n",
    "eval_agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b56ed419",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<INSTRUCTIONS>{instructions}</INSTRUCTIONS>\\n    <QUESTION>{question}</QUESTION>\\n    <ANSWER>{answer}</ANSWER>\\n    <LOG>{log}</LOG>'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt_format = \"\"\"\n",
    "    <INSTRUCTIONS>{instructions}</INSTRUCTIONS>\n",
    "    <QUESTION>{question}</QUESTION>\n",
    "    <ANSWER>{answer}</ANSWER>\n",
    "    <LOG>{log}</LOG>\n",
    "    \"\"\".strip()\n",
    "prompt_format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d1466872",
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "import json\n",
    "\n",
    "eval_set = []\n",
    "\n",
    "# Load all log records\n",
    "for log_file in LOG_DIR.glob('*.json'):\n",
    "    if 'Query_Agent' not in log_file.name:\n",
    "        continue\n",
    "    log_record = load_log_file(log_file)\n",
    "    eval_set.append(log_record)\n",
    "\n",
    "eval_results = []\n",
    "\n",
    "# Define an async function to handle the event loop\n",
    "async def evaluate_logs():\n",
    "    for log_record in eval_set:\n",
    "        # Prepare the prompt using the log_record data\n",
    "        log = json.dumps(simplify_log_messages(log_record[\"messages\"]))\n",
    "        prompt = prompt_format.format(\n",
    "            instructions=log_record[\"system_prompt\"],\n",
    "            question=log_record[\"messages\"][0][\"parts\"][0][\"content\"],\n",
    "            answer=log_record[\"messages\"][-1][\"parts\"][0][\"content\"],\n",
    "            log=log\n",
    "        )\n",
    "        \n",
    "        # Use run_in_executor to run eval_agent.run_sync in a separate thread/process\n",
    "        eval_result = await asyncio.get_event_loop().run_in_executor(\n",
    "            None,  # None means the default executor (i.e., use a thread pool)\n",
    "            lambda: eval_agent.run_sync(prompt)  # Pass only the prompt to run_sync\n",
    "        )\n",
    "        \n",
    "        eval_results.append((log_record, eval_result))\n",
    "\n",
    "# Run the async function in the existing event loop\n",
    "await evaluate_logs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0838b7bb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>file</th>\n",
       "      <th>question</th>\n",
       "      <th>answer</th>\n",
       "      <th>instructions_follow</th>\n",
       "      <th>instructions_avoid</th>\n",
       "      <th>answer_relevant</th>\n",
       "      <th>answer_clear</th>\n",
       "      <th>answer_citations</th>\n",
       "      <th>completeness</th>\n",
       "      <th>tool_call_search</th>\n",
       "      <th>uncertainty_explicitly_stated_in_strong_match_output</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Query_Agent_20260120_024020_71f106.json</td>\n",
       "      <td>If uncertainty exists, is it explicitly stated?</td>\n",
       "      <td>No strong matches were found for this query.</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Query_Agent_20260120_024356_59d1ac.json</td>\n",
       "      <td>Metric: Weighted Rowwise Root Mean Squared Error</td>\n",
       "      <td>### Strong Match\\n\\n1. **Competition Name:** O...</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Query_Agent_20260120_024910_0cc055.json</td>\n",
       "      <td>Are no required elements missing?</td>\n",
       "      <td>No strong matches were found for this query.</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Query_Agent_20260120_024721_78aa99.json</td>\n",
       "      <td>Does the answer address ALL explicit constrain...</td>\n",
       "      <td>Yes, the answer addresses all explicit constra...</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Query_Agent_20260120_024039_86d9dd.json</td>\n",
       "      <td>Are assumptions, inferences, or uncertainties ...</td>\n",
       "      <td>### Strong Match\\n\\n1. **Competition Name:** M...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>Query_Agent_20260120_024128_923022.json</td>\n",
       "      <td>Does the answer address ALL explicit constrain...</td>\n",
       "      <td>Yes, the answer addresses all explicit constra...</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>Query_Agent_20260120_024027_8d02ba.json</td>\n",
       "      <td>Are results correctly classified (Strong / Par...</td>\n",
       "      <td>No strong matches were found for this query.</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74</th>\n",
       "      <td>Query_Agent_20260120_024220_0d111d.json</td>\n",
       "      <td>Are all relevant items included?</td>\n",
       "      <td>### Strong Match\\n\\n1. **Competition Name:** E...</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75</th>\n",
       "      <td>Query_Agent_20260120_024853_1af440.json</td>\n",
       "      <td>Please provide the solutions for the Diabetic ...</td>\n",
       "      <td>### Strong Match\\n\\n1. **Competition Name:** D...</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76</th>\n",
       "      <td>Query_Agent_20260120_024245_ddb1f6.json</td>\n",
       "      <td>Is the reported number of solutions consistent...</td>\n",
       "      <td>No strong matches were found for this query.</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>77 rows × 11 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       file  \\\n",
       "0   Query_Agent_20260120_024020_71f106.json   \n",
       "1   Query_Agent_20260120_024356_59d1ac.json   \n",
       "2   Query_Agent_20260120_024910_0cc055.json   \n",
       "3   Query_Agent_20260120_024721_78aa99.json   \n",
       "4   Query_Agent_20260120_024039_86d9dd.json   \n",
       "..                                      ...   \n",
       "72  Query_Agent_20260120_024128_923022.json   \n",
       "73  Query_Agent_20260120_024027_8d02ba.json   \n",
       "74  Query_Agent_20260120_024220_0d111d.json   \n",
       "75  Query_Agent_20260120_024853_1af440.json   \n",
       "76  Query_Agent_20260120_024245_ddb1f6.json   \n",
       "\n",
       "                                             question  \\\n",
       "0     If uncertainty exists, is it explicitly stated?   \n",
       "1    Metric: Weighted Rowwise Root Mean Squared Error   \n",
       "2                   Are no required elements missing?   \n",
       "3   Does the answer address ALL explicit constrain...   \n",
       "4   Are assumptions, inferences, or uncertainties ...   \n",
       "..                                                ...   \n",
       "72  Does the answer address ALL explicit constrain...   \n",
       "73  Are results correctly classified (Strong / Par...   \n",
       "74                   Are all relevant items included?   \n",
       "75  Please provide the solutions for the Diabetic ...   \n",
       "76  Is the reported number of solutions consistent...   \n",
       "\n",
       "                                               answer instructions_follow  \\\n",
       "0        No strong matches were found for this query.                True   \n",
       "1   ### Strong Match\\n\\n1. **Competition Name:** O...                True   \n",
       "2        No strong matches were found for this query.                True   \n",
       "3   Yes, the answer addresses all explicit constra...               False   \n",
       "4   ### Strong Match\\n\\n1. **Competition Name:** M...               False   \n",
       "..                                                ...                 ...   \n",
       "72  Yes, the answer addresses all explicit constra...               False   \n",
       "73       No strong matches were found for this query.                True   \n",
       "74  ### Strong Match\\n\\n1. **Competition Name:** E...                True   \n",
       "75  ### Strong Match\\n\\n1. **Competition Name:** D...                True   \n",
       "76       No strong matches were found for this query.                True   \n",
       "\n",
       "   instructions_avoid answer_relevant answer_clear answer_citations  \\\n",
       "0                True           False         True             True   \n",
       "1                True            True         True             True   \n",
       "2                True           False         True            False   \n",
       "3                True            True        False             True   \n",
       "4               False           False        False            False   \n",
       "..                ...             ...          ...              ...   \n",
       "72               True            True         True             True   \n",
       "73               True            True         True             True   \n",
       "74               True           False        False             True   \n",
       "75               True            True         True             True   \n",
       "76               True            True         True             True   \n",
       "\n",
       "   completeness tool_call_search  \\\n",
       "0         False            False   \n",
       "1          True            False   \n",
       "2         False            False   \n",
       "3         False            False   \n",
       "4         False             True   \n",
       "..          ...              ...   \n",
       "72        False            False   \n",
       "73         True             True   \n",
       "74        False             True   \n",
       "75         True             True   \n",
       "76         True             True   \n",
       "\n",
       "   uncertainty_explicitly_stated_in_strong_match_output  \n",
       "0                                                 NaN    \n",
       "1                                                 NaN    \n",
       "2                                                 NaN    \n",
       "3                                                 NaN    \n",
       "4                                                 NaN    \n",
       "..                                                ...    \n",
       "72                                                NaN    \n",
       "73                                                NaN    \n",
       "74                                                NaN    \n",
       "75                                                NaN    \n",
       "76                                                NaN    \n",
       "\n",
       "[77 rows x 11 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "rows = []\n",
    "\n",
    "for log_record, eval_result in eval_results:\n",
    "    messages = log_record['messages']\n",
    "\n",
    "    row = {\n",
    "        'file': log_record['log_file'].name,\n",
    "        'question': messages[0]['parts'][0]['content'],\n",
    "        'answer': messages[-1]['parts'][0]['content']}\n",
    "    \n",
    "    if hasattr(eval_result, 'output') and hasattr(eval_result.output, 'checklist'):\n",
    "        checks = {c.check_name: c.check_pass for c in eval_result.output.checklist}\n",
    "    else:\n",
    "        # If checklist is not available, handle it here\n",
    "        checks = {}\n",
    "\n",
    "    row.update(checks)\n",
    "    rows.append(row)\n",
    "\n",
    "df_evals = pd.DataFrame(rows)\n",
    "df_evals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "aa8d5757",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>file</th>\n",
       "      <th>question</th>\n",
       "      <th>answer</th>\n",
       "      <th>instructions_follow</th>\n",
       "      <th>instructions_avoid</th>\n",
       "      <th>answer_relevant</th>\n",
       "      <th>answer_clear</th>\n",
       "      <th>answer_citations</th>\n",
       "      <th>completeness</th>\n",
       "      <th>tool_call_search</th>\n",
       "      <th>uncertainty_explicitly_stated_in_strong_match_output</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Query_Agent_20260120_024020_71f106.json</td>\n",
       "      <td>If uncertainty exists, is it explicitly stated?</td>\n",
       "      <td>No strong matches were found for this query.</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Query_Agent_20260120_024356_59d1ac.json</td>\n",
       "      <td>Metric: Weighted Rowwise Root Mean Squared Error</td>\n",
       "      <td>### Strong Match\\n\\n1. **Competition Name:** O...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Query_Agent_20260120_024910_0cc055.json</td>\n",
       "      <td>Are no required elements missing?</td>\n",
       "      <td>No strong matches were found for this query.</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Query_Agent_20260120_024721_78aa99.json</td>\n",
       "      <td>Does the answer address ALL explicit constrain...</td>\n",
       "      <td>Yes, the answer addresses all explicit constra...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Query_Agent_20260120_024039_86d9dd.json</td>\n",
       "      <td>Are assumptions, inferences, or uncertainties ...</td>\n",
       "      <td>### Strong Match\\n\\n1. **Competition Name:** M...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>Query_Agent_20260120_024128_923022.json</td>\n",
       "      <td>Does the answer address ALL explicit constrain...</td>\n",
       "      <td>Yes, the answer addresses all explicit constra...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>Query_Agent_20260120_024027_8d02ba.json</td>\n",
       "      <td>Are results correctly classified (Strong / Par...</td>\n",
       "      <td>No strong matches were found for this query.</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74</th>\n",
       "      <td>Query_Agent_20260120_024220_0d111d.json</td>\n",
       "      <td>Are all relevant items included?</td>\n",
       "      <td>### Strong Match\\n\\n1. **Competition Name:** E...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75</th>\n",
       "      <td>Query_Agent_20260120_024853_1af440.json</td>\n",
       "      <td>Please provide the solutions for the Diabetic ...</td>\n",
       "      <td>### Strong Match\\n\\n1. **Competition Name:** D...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76</th>\n",
       "      <td>Query_Agent_20260120_024245_ddb1f6.json</td>\n",
       "      <td>Is the reported number of solutions consistent...</td>\n",
       "      <td>No strong matches were found for this query.</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>77 rows × 11 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       file  \\\n",
       "0   Query_Agent_20260120_024020_71f106.json   \n",
       "1   Query_Agent_20260120_024356_59d1ac.json   \n",
       "2   Query_Agent_20260120_024910_0cc055.json   \n",
       "3   Query_Agent_20260120_024721_78aa99.json   \n",
       "4   Query_Agent_20260120_024039_86d9dd.json   \n",
       "..                                      ...   \n",
       "72  Query_Agent_20260120_024128_923022.json   \n",
       "73  Query_Agent_20260120_024027_8d02ba.json   \n",
       "74  Query_Agent_20260120_024220_0d111d.json   \n",
       "75  Query_Agent_20260120_024853_1af440.json   \n",
       "76  Query_Agent_20260120_024245_ddb1f6.json   \n",
       "\n",
       "                                             question  \\\n",
       "0     If uncertainty exists, is it explicitly stated?   \n",
       "1    Metric: Weighted Rowwise Root Mean Squared Error   \n",
       "2                   Are no required elements missing?   \n",
       "3   Does the answer address ALL explicit constrain...   \n",
       "4   Are assumptions, inferences, or uncertainties ...   \n",
       "..                                                ...   \n",
       "72  Does the answer address ALL explicit constrain...   \n",
       "73  Are results correctly classified (Strong / Par...   \n",
       "74                   Are all relevant items included?   \n",
       "75  Please provide the solutions for the Diabetic ...   \n",
       "76  Is the reported number of solutions consistent...   \n",
       "\n",
       "                                               answer  instructions_follow  \\\n",
       "0        No strong matches were found for this query.                  1.0   \n",
       "1   ### Strong Match\\n\\n1. **Competition Name:** O...                  1.0   \n",
       "2        No strong matches were found for this query.                  1.0   \n",
       "3   Yes, the answer addresses all explicit constra...                  0.0   \n",
       "4   ### Strong Match\\n\\n1. **Competition Name:** M...                  0.0   \n",
       "..                                                ...                  ...   \n",
       "72  Yes, the answer addresses all explicit constra...                  0.0   \n",
       "73       No strong matches were found for this query.                  1.0   \n",
       "74  ### Strong Match\\n\\n1. **Competition Name:** E...                  1.0   \n",
       "75  ### Strong Match\\n\\n1. **Competition Name:** D...                  1.0   \n",
       "76       No strong matches were found for this query.                  1.0   \n",
       "\n",
       "    instructions_avoid  answer_relevant  answer_clear  answer_citations  \\\n",
       "0                  1.0              0.0           1.0               1.0   \n",
       "1                  1.0              1.0           1.0               1.0   \n",
       "2                  1.0              0.0           1.0               0.0   \n",
       "3                  1.0              1.0           0.0               1.0   \n",
       "4                  0.0              0.0           0.0               0.0   \n",
       "..                 ...              ...           ...               ...   \n",
       "72                 1.0              1.0           1.0               1.0   \n",
       "73                 1.0              1.0           1.0               1.0   \n",
       "74                 1.0              0.0           0.0               1.0   \n",
       "75                 1.0              1.0           1.0               1.0   \n",
       "76                 1.0              1.0           1.0               1.0   \n",
       "\n",
       "    completeness  tool_call_search  \\\n",
       "0            0.0               0.0   \n",
       "1            1.0               0.0   \n",
       "2            0.0               0.0   \n",
       "3            0.0               0.0   \n",
       "4            0.0               1.0   \n",
       "..           ...               ...   \n",
       "72           0.0               0.0   \n",
       "73           1.0               1.0   \n",
       "74           0.0               1.0   \n",
       "75           1.0               1.0   \n",
       "76           1.0               1.0   \n",
       "\n",
       "   uncertainty_explicitly_stated_in_strong_match_output  \n",
       "0                                                 NaN    \n",
       "1                                                 NaN    \n",
       "2                                                 NaN    \n",
       "3                                                 NaN    \n",
       "4                                                 NaN    \n",
       "..                                                ...    \n",
       "72                                                NaN    \n",
       "73                                                NaN    \n",
       "74                                                NaN    \n",
       "75                                                NaN    \n",
       "76                                                NaN    \n",
       "\n",
       "[77 rows x 11 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_cols = [\n",
    "    'instructions_follow',\n",
    "    'instructions_avoid',\n",
    "    'answer_relevant',\n",
    "    'answer_clear',\n",
    "    'answer_citations',\n",
    "    'completeness',\n",
    "    'tool_call_search']\n",
    "\n",
    "df_evals[eval_cols] = df_evals[eval_cols].apply(lambda col: col.map({True: 1, False: 0}))\n",
    "df_evals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ce77e70a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluate performance of AI Agent\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "instructions_follow    72.368421\n",
       "instructions_avoid     92.105263\n",
       "answer_relevant        51.315789\n",
       "answer_clear           65.789474\n",
       "answer_citations       90.789474\n",
       "completeness           44.736842\n",
       "tool_call_search       51.315789\n",
       "dtype: float64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('Evaluate performance of AI Agent')\n",
    "df_evals[eval_cols].mean() * 100"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (app uv)",
   "language": "python",
   "name": "app-uv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
