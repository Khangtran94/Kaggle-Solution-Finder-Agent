{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "69acd3c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/workspaces/Kaggle-Solution-Finder-Agent/app/.venv/bin/python\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "print(sys.executable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0a0e8c52",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspaces/Kaggle-Solution-Finder-Agent/app/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# 1) Redirect logs to eval_logs/\n",
    "os.environ[\"LOGS_DIRECTORY\"] = \"eval_logs\"\n",
    "\n",
    "# 2) Make app/ importable\n",
    "REPO_ROOT = Path.cwd().parent\n",
    "sys.path.insert(0, str(REPO_ROOT / \"app\"))\n",
    "\n",
    "from ingest import (read_repo_data,extract_completed_competitions,build_vector_index)\n",
    "from search_agent import create_search_agent\n",
    "from logs import log_interaction_to_file, LOG_DIR\n",
    "from pydantic_ai import Agent\n",
    "from logs import EvaluationChecklist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "91356668",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PosixPath('/workspaces/Kaggle-Solution-Finder-Agent/eval/logs')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "LOG_DIR.absolute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "264870dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 18/18 [01:06<00:00,  3.68s/it]\n"
     ]
    }
   ],
   "source": [
    "project_docs = read_repo_data(repo_owner=\"Khangtran94\",repo_name=\"kaggle-solutions\",branch=\"gh-pages\")\n",
    "completed_competitions = extract_completed_competitions(project_docs)\n",
    "embedding_model, vindex = build_vector_index(completed_competitions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2e60aa08",
   "metadata": {},
   "outputs": [],
   "source": [
    "question_generation_prompt = \"\"\"\n",
    "You are an evaluation agent responsible for assessing the quality, correctness,\n",
    "    and consistency of an AI assistant’s response to a user query.\n",
    "\n",
    "    INPUTS YOU WILL RECEIVE:\n",
    "    1. The original user query\n",
    "    2. The system prompt used by the assistant\n",
    "    3. The assistant’s final answer\n",
    "\n",
    "    YOUR TASK:\n",
    "    Evaluate whether the assistant’s answer satisfies the system prompt and the user query.\n",
    "\n",
    "    EVALUATION CRITERIA (mandatory):\n",
    "\n",
    "    1. QUERY SATISFACTION\n",
    "    - Does the answer address ALL explicit constraints in the query?\n",
    "    - Are numeric conditions (e.g., “more than 5 solutions”) respected?\n",
    "    - Are no required elements missing?\n",
    "\n",
    "    2. COMPLETENESS\n",
    "    - Are all relevant items included?\n",
    "    - Were any potential matches silently omitted?\n",
    "    - If uncertainty exists, is it explicitly stated?\n",
    "\n",
    "    3. CONSISTENCY & DETERMINISM\n",
    "    - Would the same query reasonably yield the same set of results?\n",
    "    - Is the ordering stable and logically justified?\n",
    "\n",
    "    4. DOMAIN RELEVANCE\n",
    "    - Are results correctly classified (Strong / Partial / Weak match)?\n",
    "    - Are irrelevant items incorrectly included or relevant ones excluded?\n",
    "\n",
    "    5. SOLUTION LISTING ACCURACY\n",
    "    - Is the reported number of solutions consistent with the listed solutions?\n",
    "    - Are links plausible and correctly associated with the competition?\n",
    "    - Are all available solutions listed when required by the system prompt?\n",
    "\n",
    "    6. TRANSPARENCY\n",
    "    - Are assumptions, inferences, or uncertainties clearly disclosed?\n",
    "    - Is there any hallucinated or unverifiable information?\n",
    "\n",
    "    OUTPUT FORMAT (STRICT):\n",
    "    Provide:\n",
    "    - A verdict: PASS or FAIL\n",
    "    - A brief justification for each evaluation criterion\n",
    "    - A final summary highlighting the most critical issues (if any)\n",
    "\n",
    "    DO NOT:\n",
    "    - Rewrite the assistant’s answer\n",
    "    - Introduce new competitions or facts\n",
    "    - Assume missing information is correct\n",
    "\n",
    "    Your role is to judge, not to fix.\n",
    "\"\"\".strip()\n",
    "\n",
    "from pydantic import BaseModel\n",
    "from pydantic_ai import Agent\n",
    "\n",
    "class QuestionsList(BaseModel):\n",
    "    questions: list[str]\n",
    "\n",
    "question_generate_agent = Agent(\n",
    "    name=\"question_generate_agent\",\n",
    "    instructions=question_generation_prompt,\n",
    "    model=\"gpt-4o-mini\",\n",
    "    output_type=QuestionsList)\n",
    "\n",
    "import random\n",
    "import json\n",
    "# Sample 10 random completed competitions from the list\n",
    "sample = random.sample(completed_competitions, 10)\n",
    "\n",
    "# # Create the prompt docs\n",
    "# prompt_docs = [\n",
    "#     (\n",
    "#         f\"Title: {d.get('title', 'N/A')}\\n\"\n",
    "#         f\"Metric: {d.get('metric', 'N/A')}\\n\"\n",
    "#         f\"Link: {d.get('link', 'N/A')}\\n\"\n",
    "#         f\"Number of Solutions: {len(d['solutions']) if isinstance(d.get('solutions'), list) else d.get('solutions', 'N/A')}\\n\"\n",
    "#     )\n",
    "#     for d in sample\n",
    "# ]\n",
    "\n",
    "# # Join the formatted prompt docs into a single string\n",
    "# prompt = \"\\n\\n\".join(prompt_docs)\n",
    "# prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2d802b6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "questions = []\n",
    "\n",
    "for competition in sample:\n",
    "    # Build prompt once per competition\n",
    "    prompt = (\n",
    "        f\"Title: {competition.get('title', 'N/A')}\\n\"\n",
    "        f\"Metric: {competition.get('metric', 'N/A')}\\n\"\n",
    "        f\"Link: {competition.get('link', 'N/A')}\\n\"\n",
    "        f\"Number of Solutions: \"\n",
    "        f\"{len(competition['solutions']) if isinstance(competition.get('solutions'), list) else competition.get('solutions', 'N/A')}\\n\"\n",
    "    )\n",
    "\n",
    "    # Generate questions\n",
    "    result = await question_generate_agent.run(prompt)\n",
    "\n",
    "    # Collect questions\n",
    "    questions.extend(result.output.questions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cfd996c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40\n",
      "['What does the Abstraction and Reasoning Challenge evaluation metric measure in the ARC Prize 2024 competition and what techniques could participants use to optimize it?', \"How does the title 'ARC Prize 2024' reflect the underlying tasks and goals of this competition?\", 'What are the key domains and topics covered in the ARC Prize 2024 competition, and how do they relate to artificial intelligence and reasoning?', 'With only 7 solutions submitted in the ARC Prize 2024 competition, what competitive strategies might impact the performance and diversity of approaches among participants?', 'What does the Weighted Multiclass Loss metric measure, and how can participants optimize their models to achieve a lower value for this metric in the Mayo Clinic - STRIP AI competition?', \"How does the title 'Mayo Clinic - STRIP AI' reflect the focus of the project, and what implications does it have for the types of solutions being submitted?\", 'What medical domain challenges are being addressed in the Mayo Clinic - STRIP AI competition, and how do they relate to the objectives of the project?', 'Considering there are 10 solutions submitted for the competition, how might the diversity in these solutions impact the learning outcomes and potential approaches for future participants?', 'What does the Area Under Receiver Operating Characteristic Curve (AUC-ROC) measure in this competition, and what strategies can you employ to maximize this metric?', \"How does the title 'Playground Series - Season 3, Episode 7' reflect the format or series theme of Kaggle competitions?\", 'In what domain does this competition reside, and how does it relate to real-world applications or similar datasets?', 'With 6 solutions submitted in this competition, what key factors might influence the number of participants and the diversity of solutions provided?', 'What does the Root Mean Squared Logarithmic Error (RMSLE) measure in the context of forecasting bike sharing demand, and why is it particularly suitable for this competition?', \"How does the title 'Bike Sharing Demand' reflect the primary concern that participants will need to address while developing their models?\", 'In what ways does the bike sharing domain leverage data science and machine learning to improve service utilization and customer experience?', 'Considering that there is only 1 solution submitted for this competition, what might this indicate about the level of community engagement or the complexity of the challenge?', 'What does the Categorization Accuracy metric measure in the context of predicting purchases in the Allstate Purchase Prediction Challenge, and what techniques can be used to optimize this metric?', \"How does the title 'Allstate Purchase Prediction Challenge' reflect the primary objective and focus of the competition?\", 'What domain does the Allstate Purchase Prediction Challenge belong to, and why is this domain significant for machine learning applications?', \"Given that there are 2 solutions submitted in this competition, how can participants leverage these existing solutions to improve their own model's performance?\", 'What is Categorization Accuracy measuring in the context of the Facial Expression Recognition Challenge, and what strategies can participants employ to optimize this metric?', \"How does the title 'Challenges in Representation Learning; Facial Expression Recognition Challenge' reflect the main objectives or focal areas of the competition?\", 'What unique challenges are associated with the domain of facial expression recognition, and how might these challenges affect model performance in this competition?', 'Given that there are only 2 solutions submitted for this challenge, what factors might influence participant engagement and solution submissions in Kaggle competitions?', 'What does the Bidirectional AUC metric specifically measure in the context of determining cause-effect pairs, and how can participants optimize their models to improve this score?', \"How does the project titled 'Cause-effect pairs' relate to various real-world situations or scientific domains, and what are some examples of cause-effect pairs that could be explored?\", \"In which domain does the 'Cause-effect pairs' competition primarily operate, and what importance do these cause-effect relationships hold within this domain?\", \"With only 1 solution submitted for the 'Cause-effect pairs' competition, what might this imply about the level of interest and competition among participants, and could this affect the robustness of the solutions developed?\", 'What does the evaluation metric 58266_TpuGraphsEval measure in the context of predicting AI model runtime, and how does it impact model performance?', \"How does the title 'Google - Fast or Slow? Predict AI Model Runtime' reflect the main objective of the competition, and why is runtime prediction important in AI?\", 'What specific challenges do participants face in the domain of AI model runtime prediction, and which techniques are most commonly applied to tackle these challenges?', 'With 16 solutions submitted, what can participants learn from analyzing the diversity of approaches taken in this competition?', 'What does the Root Mean Squared Error metric measure in the context of this competition, and how can it be minimized effectively?', 'What are the key factors contributing to customer revenue prediction as indicated in the title of the project?', 'In what ways does the domain of Google Analytics influence the modeling techniques used in the Customer Revenue Prediction competition?', 'Are the 4 solutions submitted for this competition indicative of a diverse range of approaches or techniques, and how might that affect the overall learning from this competition?', 'What does the Image Matching Challenge pose mAA metric measure, and what strategies can be used to optimize this metric in the context of the competition?', \"How does the title 'Image Matching Challenge 2022' reflect the objectives and tasks participants will tackle in this competition?\", 'What is the primary domain of the Image Matching Challenge 2022, and what real-world applications stem from advancements in this area?', 'With 19 solutions submitted, what insights can be drawn regarding the diversity and competitiveness of approaches taken by participants in the Image Matching Challenge 2022?']\n"
     ]
    }
   ],
   "source": [
    "# # Generate questions for each competition individually\n",
    "# questions = []\n",
    "\n",
    "# # Loop through each competition in the sample\n",
    "# for competition in sample:\n",
    "#     # Create the individual prompt for the competition\n",
    "#     prompt = (\n",
    "#         f\"Title: {competition.get('title', 'N/A')}\\n\"\n",
    "#         f\"Metric: {competition.get('metric', 'N/A')}\\n\"\n",
    "#         f\"Link: {competition.get('link', 'N/A')}\\n\"\n",
    "#         f\"Number of Solutions: {len(competition['solutions']) if isinstance(competition.get('solutions'), list) else competition.get('solutions', 'N/A')}\\n\"\n",
    "#     )\n",
    "    \n",
    "#     # Generate the questions for this competition\n",
    "#     question_generated = await question_generate_agent.run(prompt)\n",
    "    \n",
    "#     # Append the questions to the final list\n",
    "#     questions.extend(question_generated.output.questions)\n",
    "\n",
    "\n",
    "# # Print the questions to check them\n",
    "# print(questions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "92d8984a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"Does the assistant's answer satisfy the system prompt regarding the 'Carvana Image Masking Challenge' by providing the correct title, metric, link, and number of solutions?\",\n",
       " \"Does the assistant's answer explicitly address all constraints in the user query?\",\n",
       " \"Are all relevant items included in the assistant's response?\",\n",
       " 'Is the ordering of items stable and logically justified?',\n",
       " 'Are results correctly classified as Strong / Partial / Weak match with no irrelevant items included?',\n",
       " 'Is the reported number of solutions consistent with the listed solutions?',\n",
       " 'Are all available solutions listed as required by the system prompt?',\n",
       " 'Are assumptions, inferences, or uncertainties clearly disclosed?',\n",
       " \"Is there any unverifiable information present in the assistant's response?\",\n",
       " 'PASS',\n",
       " \"The assistant correctly identified one solution as required by the user query in connection to the 'NFL 1st and Future - Analytics.'\",\n",
       " \"Does the assistant's answer address ALL explicit constraints in the query?\",\n",
       " \"Are numeric conditions (e.g., 'more than 5 solutions') respected?\",\n",
       " 'Are no required elements missing?',\n",
       " 'Are all relevant items included?',\n",
       " 'Were any potential matches silently omitted?',\n",
       " 'If uncertainty exists, is it explicitly stated?',\n",
       " 'Would the same query reasonably yield the same set of results?',\n",
       " 'Is the ordering stable and logically justified?',\n",
       " 'Are results correctly classified (Strong / Partial / Weak match)?',\n",
       " 'Are irrelevant items incorrectly included or relevant ones excluded?',\n",
       " 'Is the reported number of solutions consistent with the listed solutions?',\n",
       " 'Are links plausible and correctly associated with the competition?',\n",
       " 'Are all available solutions listed when required by the system prompt?',\n",
       " 'Are assumptions, inferences, or uncertainties clearly disclosed?',\n",
       " 'Is there any hallucinated or unverifiable information?',\n",
       " 'The original user query',\n",
       " 'The system prompt used by the assistant',\n",
       " 'The assistant’s final answer',\n",
       " 'Does the answer address ALL explicit constraints in the query?',\n",
       " 'Are numeric conditions (e.g., “more than 5 solutions”) respected?',\n",
       " 'Are no required elements missing?',\n",
       " 'Are all relevant items included?',\n",
       " 'Were any potential matches silently omitted?',\n",
       " 'If uncertainty exists, is it explicitly stated?',\n",
       " 'Would the same query reasonably yield the same set of results?',\n",
       " 'Is the ordering stable and logically justified?',\n",
       " 'Are results correctly classified (Strong / Partial / Weak match)?',\n",
       " 'Are irrelevant items incorrectly included or relevant ones excluded?',\n",
       " 'Is the reported number of solutions consistent with the listed solutions?',\n",
       " 'Are links plausible and correctly associated with the competition?',\n",
       " 'Are all available solutions listed when required by the system prompt?',\n",
       " 'Are assumptions, inferences, or uncertainties clearly disclosed?',\n",
       " 'Is there any hallucinated or unverifiable information?',\n",
       " 'Title: Open Problems – Single-Cell Perturbations',\n",
       " 'Metric: Weighted Rowwise Root Mean Squared Error',\n",
       " 'Link: https://www.kaggle.com/c/open-problems-single-cell-perturbations',\n",
       " 'Number of Solutions: 9',\n",
       " 'PASS',\n",
       " \"The assistant's response includes all necessary details from the query, including the title, metric, link, and number of solutions which is explicitly stated. Each element of the query is respected. \",\n",
       " 'The answer includes all relevant items without omitting any potential matches, and it accurately reflects the contents of the user query without generating additional uncertainty.',\n",
       " 'The response is deterministic; the same query would yield the same result and the ordering follows the structure provided in the user query. ',\n",
       " 'The results are correctly classified as a strong match to the query with no irrelevant items included or relevant ones omitted.',\n",
       " 'The reported number of solutions (2) is consistent with the solutions listed, and the link is accurately associated with the competition. Since the system prompt does not ask for additional solutions, the assistant has met requirements. ',\n",
       " 'Assumptions and uncertainties about the content are minimal, and no hallucinated or unverifiable information is present.',\n",
       " 'Does the answer address ALL explicit constraints in the query? Are numeric conditions respected? Are no required elements missing?',\n",
       " 'Are all relevant items included? Were any potential matches silently omitted?',\n",
       " 'Would the same query reasonably yield the same set of results? Is the ordering stable and logically justified?',\n",
       " 'Are results correctly classified (Strong / Partial / Weak match)? Are irrelevant items incorrectly included or relevant ones excluded?',\n",
       " 'Is the reported number of solutions consistent with the listed solutions? Are links plausible and correctly associated with the competition? Are all available solutions listed when required by the system prompt?',\n",
       " 'Are assumptions, inferences, or uncertainties clearly disclosed? Is there any hallucinated or unverifiable information?',\n",
       " 'Please provide the solutions for the Diabetic Retinopathy Detection metric: QuadraticWeightedKappa. The solutions should include a link for each and the total number of solutions should be six.',\n",
       " 'Does the answer address ALL explicit constraints in the query?',\n",
       " 'Are numeric conditions respected?',\n",
       " 'Are no required elements missing?',\n",
       " 'Are all relevant items included?',\n",
       " 'Were any potential matches silently omitted?',\n",
       " 'Is uncertainty explicitly stated?',\n",
       " 'Would the same query reasonably yield the same set of results?',\n",
       " 'Is the ordering stable and logically justified?',\n",
       " 'Are results correctly classified (Strong / Partial / Weak match)?',\n",
       " 'Are irrelevant items incorrectly included or relevant ones excluded?',\n",
       " 'Is the reported number of solutions consistent with the listed solutions?',\n",
       " 'Are links plausible and correctly associated with the competition?',\n",
       " 'Are all available solutions listed when required by the system prompt?',\n",
       " 'Are assumptions, inferences, or uncertainties clearly disclosed?',\n",
       " 'Is there any hallucinated or unverifiable information?']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "efe7a6f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Does the assistant's answer satisfy the system prompt regarding the 'Carvana Image Masking Challenge' by providing the correct title, metric, link, and number of solutions?\n",
      "Does the assistant's answer explicitly address all constraints in the user query?\n",
      "Are all relevant items included in the assistant's response?\n",
      "Is the ordering of items stable and logically justified?\n",
      "Are results correctly classified as Strong / Partial / Weak match with no irrelevant items included?\n",
      "Is the reported number of solutions consistent with the listed solutions?\n",
      "Are all available solutions listed as required by the system prompt?\n",
      "Are assumptions, inferences, or uncertainties clearly disclosed?\n",
      "Is there any unverifiable information present in the assistant's response?\n",
      "PASS\n",
      "The assistant correctly identified one solution as required by the user query in connection to the 'NFL 1st and Future - Analytics.'\n",
      "Does the assistant's answer address ALL explicit constraints in the query?\n",
      "Are numeric conditions (e.g., 'more than 5 solutions') respected?\n",
      "Are no required elements missing?\n",
      "Are all relevant items included?\n",
      "Were any potential matches silently omitted?\n",
      "If uncertainty exists, is it explicitly stated?\n",
      "Would the same query reasonably yield the same set of results?\n",
      "Is the ordering stable and logically justified?\n",
      "Are results correctly classified (Strong / Partial / Weak match)?\n",
      "Are irrelevant items incorrectly included or relevant ones excluded?\n",
      "Is the reported number of solutions consistent with the listed solutions?\n",
      "Are links plausible and correctly associated with the competition?\n",
      "Are all available solutions listed when required by the system prompt?\n",
      "Are assumptions, inferences, or uncertainties clearly disclosed?\n",
      "Is there any hallucinated or unverifiable information?\n",
      "The original user query\n",
      "The system prompt used by the assistant\n",
      "The assistant’s final answer\n",
      "Does the answer address ALL explicit constraints in the query?\n",
      "Are numeric conditions (e.g., “more than 5 solutions”) respected?\n",
      "Are no required elements missing?\n",
      "Are all relevant items included?\n",
      "Were any potential matches silently omitted?\n",
      "If uncertainty exists, is it explicitly stated?\n",
      "Would the same query reasonably yield the same set of results?\n",
      "Is the ordering stable and logically justified?\n",
      "Are results correctly classified (Strong / Partial / Weak match)?\n",
      "Are irrelevant items incorrectly included or relevant ones excluded?\n",
      "Is the reported number of solutions consistent with the listed solutions?\n",
      "Are links plausible and correctly associated with the competition?\n",
      "Are all available solutions listed when required by the system prompt?\n",
      "Are assumptions, inferences, or uncertainties clearly disclosed?\n",
      "Is there any hallucinated or unverifiable information?\n",
      "Title: Open Problems – Single-Cell Perturbations\n",
      "Metric: Weighted Rowwise Root Mean Squared Error\n",
      "Link: https://www.kaggle.com/c/open-problems-single-cell-perturbations\n",
      "Number of Solutions: 9\n",
      "PASS\n",
      "The assistant's response includes all necessary details from the query, including the title, metric, link, and number of solutions which is explicitly stated. Each element of the query is respected. \n",
      "The answer includes all relevant items without omitting any potential matches, and it accurately reflects the contents of the user query without generating additional uncertainty.\n",
      "The response is deterministic; the same query would yield the same result and the ordering follows the structure provided in the user query. \n",
      "The results are correctly classified as a strong match to the query with no irrelevant items included or relevant ones omitted.\n",
      "The reported number of solutions (2) is consistent with the solutions listed, and the link is accurately associated with the competition. Since the system prompt does not ask for additional solutions, the assistant has met requirements. \n",
      "Assumptions and uncertainties about the content are minimal, and no hallucinated or unverifiable information is present.\n",
      "Does the answer address ALL explicit constraints in the query? Are numeric conditions respected? Are no required elements missing?\n",
      "Are all relevant items included? Were any potential matches silently omitted?\n",
      "Would the same query reasonably yield the same set of results? Is the ordering stable and logically justified?\n",
      "Are results correctly classified (Strong / Partial / Weak match)? Are irrelevant items incorrectly included or relevant ones excluded?\n",
      "Is the reported number of solutions consistent with the listed solutions? Are links plausible and correctly associated with the competition? Are all available solutions listed when required by the system prompt?\n",
      "Are assumptions, inferences, or uncertainties clearly disclosed? Is there any hallucinated or unverifiable information?\n",
      "Please provide the solutions for the Diabetic Retinopathy Detection metric: QuadraticWeightedKappa. The solutions should include a link for each and the total number of solutions should be six.\n",
      "Does the answer address ALL explicit constraints in the query?\n",
      "Are numeric conditions respected?\n",
      "Are no required elements missing?\n",
      "Are all relevant items included?\n",
      "Were any potential matches silently omitted?\n",
      "Is uncertainty explicitly stated?\n",
      "Would the same query reasonably yield the same set of results?\n",
      "Is the ordering stable and logically justified?\n",
      "Are results correctly classified (Strong / Partial / Weak match)?\n",
      "Are irrelevant items incorrectly included or relevant ones excluded?\n",
      "Is the reported number of solutions consistent with the listed solutions?\n",
      "Are links plausible and correctly associated with the competition?\n",
      "Are all available solutions listed when required by the system prompt?\n",
      "Are assumptions, inferences, or uncertainties clearly disclosed?\n",
      "Is there any hallucinated or unverifiable information?\n"
     ]
    }
   ],
   "source": [
    "query_agent = create_search_agent(embedding_model, vindex) \n",
    "query_agent \n",
    "for q in questions: \n",
    "    print(q) \n",
    "    answer_question = await query_agent.run(user_prompt = q) \n",
    "    log_interaction_to_file(query_agent, answer_question.new_messages())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (app uv)",
   "language": "python",
   "name": "app-uv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
